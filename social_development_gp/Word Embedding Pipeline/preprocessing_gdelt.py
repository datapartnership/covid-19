"""
Module to do preprocessing on GDELT data which already generated by Alicia
Developed by :
    Louis Owen (http://louisowen6.github.io/)
"""

import random
import re
import argparse
import pandas as pd
import numpy as np
import json
import string
from tqdm import tqdm


def get_args():
    parser = argparse.ArgumentParser(description='Data Preprocessing')
    parser.add_argument('--start_date', type=int, required=True, help='Starting Date')
    parser.add_argument('--start_month', type=int, required=True, help='Starting Month')
    return parser.parse_args()


def main():
    args = get_args()

    print('------------- Importing Previous preprocessed Data')
    df_before = pd.read_csv('/mnt/louis/Dataset/GDELT/preprocessed_gdelt.csv')
    df_before['date'] = pd.to_datetime(df_before['date'])
    df_before = df_before.sort_values(by='date')
    df_before = df_before[df_before['date']<datetime.datetime(2020,args.start_month,args.start_date)].reset_index(drop=True)


    print('------------- Importing New Unprocessed Data')
    df = pd.read_csv('/mnt/alicia/indonesian_news/clean_indonesia_news.csv',usecols=['date','text','URL'])
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(by='date')
    df = df[df['date']>=datetime.datetime(2020,args.start_month,args.start_date)].reset_index(drop=True)


    print('------------- Cleaning News Title')
    df['cleaned_text'] = df['text'].progress_apply(lambda x: sentences_cleaner(x))
    df = df[df['cleaned_text']!='']


    # print('------------- Extracting News Summary')
    # df['text_4_sentences'] = df['URL'].progress_apply(lambda x: extract_summary(x))
    # df = df[df['text_4_sentences']!='']


    print('------------- Appending Dataframe')
    df = df[['date','cleaned_text']] 
    df_all = df_before.append(df, ignore_index = True)


    df_all.to_csv('/mnt/louis/Dataset/GDELT/preprocessed_gdelt.csv',index=False)


def deEmojify(inputString):
    '''
    Function to remove emoji
    '''
    return inputString.encode('ascii', 'ignore').decode('ascii')


def sentences_cleaner(sentence):
    '''
    clean input sentence  
    '''
    try:
        #Remove Emoji
        stripped = deEmojify(sentence)

        #Remove linebreak
        stripped = re.sub('\n','',stripped)

        #Remove Punctuation
        stripped = [re.sub(r'[^\w\s]',' ',x) for x in stripped.split(string.punctuation)][0]

        #Remove Non Alphabet and Non Number Characters
        stripped = re.sub(' +',' ',re.sub(r'[^a-zA-Z-0-9]',' ',stripped)).strip()

        #Lowercase 
        stripped = stripped.lower()

        #remove stop words
        lst = pd.Series(stripped.split()).apply(lambda x: 'stopword' if x in stop_words_list else x).to_list()
        lst = [wrd for wrd in lst if wrd!='stopword']
        stripped = ' '.join(lst)

        return re.sub(' +',' ',stripped).strip()

    except Exception as e:
        print(e)
        print(sentence)
        return ''


with open("/mnt/louis/Supporting Files/combined_stop_words.txt") as f:
    stop_words_list = f.read().splitlines()


if __name__ == '__main__':
    main()